{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4692,
     "status": "ok",
     "timestamp": 1625033015308,
     "user": {
      "displayName": "Yueun Kim",
      "photoUrl": "",
      "userId": "17304567506823421488"
     },
     "user_tz": -540
    },
    "id": "iIndTUMgYuWc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import librosa\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhWQnasNYuWf"
   },
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-rtCgGdeYuWf"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "africa_train_paths = glob(\"./open/train/africa/*.wav\")\n",
    "australia_train_paths = glob(\"./open/train/australia/*.wav\")\n",
    "canada_train_paths = glob(\"./open/train/canada/*.wav\")\n",
    "england_train_paths = glob(\"./open/train/england/*.wav\")\n",
    "hongkong_train_paths = glob(\"./open/train/hongkong/*.wav\")\n",
    "us_train_paths = glob(\"./open/train/us/*.wav\")\n",
    "\n",
    "path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n",
    "             england_train_paths, hongkong_train_paths, us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BL9REY9kYuWg",
    "outputId": "2af1ee2d-c80b-4042-c0a1-c0294b0c8aa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./open/test\\1.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./open/test\\10.wav</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./open/test\\100.wav</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./open/test\\1000.wav</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./open/test\\1001.wav</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   path    id\n",
       "0     ./open/test\\1.wav     1\n",
       "1    ./open/test\\10.wav    10\n",
       "2   ./open/test\\100.wav   100\n",
       "3  ./open/test\\1000.wav  1000\n",
       "4  ./open/test\\1001.wav  1001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glob로 test data의 path를 불러올때 순서대로 로드되지 않을 경우를 주의해야 합니다.\n",
    "# test_ 데이터 프레임을 만들어서 나중에 sample_submission과 id를 기준으로 merge시킬 준비를 합니다.\n",
    "\n",
    "def get_id(data):\n",
    "#     return np.int(data.split(\"\\\\\")[1].split(\".\")[0])\n",
    "    return np.int(data.split(\"//\")[2].split(\".\")[0])\n",
    "\n",
    "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
    "test_[\"path\"] = glob(\"./open/test/*.wav\")\n",
    "test_[\"id\"] = test_[\"path\"].apply(lambda x : get_id(x))\n",
    "\n",
    "test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBrYYX8DYuWg"
   },
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1cmKVwiYuWh"
   },
   "source": [
    "baseline 코드에서는 librosa 라이브러리를 사용하여 wav파일을 전처리 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1625033575854,
     "user": {
      "displayName": "Yueun Kim",
      "photoUrl": "",
      "userId": "17304567506823421488"
     },
     "user_tz": -540
    },
    "id": "SS0do1BPYuWh"
   },
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "    result = np.array(result) \n",
    "    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "error",
     "timestamp": 1625033589249,
     "user": {
      "displayName": "Yueun Kim",
      "photoUrl": "",
      "userId": "17304567506823421488"
     },
     "user_tz": -540
    },
    "id": "fpcOfL_AYuWh",
    "outputId": "03249e4c-b4c3-4e97-f119-b164216d8d11"
   },
   "outputs": [],
   "source": [
    "# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n",
    "# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n",
    "\n",
    "# os.mkdir(\"./npy_data\")\n",
    "\n",
    "# africa_train_data = load_data(africa_train_paths)\n",
    "# np.save(\"./npy_data/africa_npy\", africa_train_data)\n",
    "\n",
    "# australia_train_data = load_data(australia_train_paths)\n",
    "# np.save(\"./npy_data/australia_npy\", australia_train_data)\n",
    "\n",
    "# canada_train_data = load_data(canada_train_paths)\n",
    "# np.save(\"./npy_data/canada_npy\", canada_train_data)\n",
    "\n",
    "# england_train_data = load_data(england_train_paths)\n",
    "# np.save(\"./npy_data/england_npy\", england_train_data)\n",
    "\n",
    "# hongkong_train_data = load_data(hongkong_train_paths)\n",
    "# np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n",
    "\n",
    "# us_train_data = load_data(us_train_paths)\n",
    "# np.save(\"./npy_data/us_npy\", us_train_data)\n",
    "\n",
    "# test_data = load_data(test_[\"path\"])\n",
    "# np.save(\"./npy_data/test_npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ne8xsWHxYuWi"
   },
   "outputs": [],
   "source": [
    "# npy파일로 저장된 데이터를 불러옵니다.\n",
    "africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle = True)\n",
    "australia_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n",
    "canada_train_data = np.load(\"./npy_data/canada_npy.npy\", allow_pickle = True)\n",
    "england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle = True)\n",
    "hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
    "us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle = True)\n",
    "\n",
    "test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle = True)\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Af9n3wLWYuWi"
   },
   "outputs": [],
   "source": [
    "# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n",
    "# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n",
    "\n",
    "def get_mini(data):\n",
    "\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "\n",
    "    return mini\n",
    "\n",
    "#음성들의 길이를 맞춰줍니다.\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "#feature를 생성합니다.\n",
    "\n",
    "def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in data:\n",
    "        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
    "        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
    "        # n_mels 는 적용할 mel filter의 개수입니다.\n",
    "        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "\n",
    "    mel_mean = mel.mean()\n",
    "    mel_std = mel.std()\n",
    "    mel = (mel - mel_mean) / mel_std\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Gzw0ZJEBYuWj"
   },
   "outputs": [],
   "source": [
    "train_x = np.concatenate(train_data_list, axis= 0)\n",
    "test_x = np.array(test_data)\n",
    "\n",
    "# 음성의 길이 중 가장 작은 길이를 구합니다.\n",
    "\n",
    "train_mini = get_mini(train_x)\n",
    "test_mini = get_mini(test_x)\n",
    "\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n",
    "\n",
    "train_x = set_length(train_x, mini)\n",
    "test_x = set_length(test_x, mini)\n",
    "\n",
    "# librosa를 이용해 feature를 추출합니다.\n",
    "\n",
    "train_x = get_feature(data = train_x)\n",
    "test_x = get_feature(data = test_x)\n",
    "\n",
    "train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n",
    "test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mioZJR3BYuWk"
   },
   "outputs": [],
   "source": [
    "# train_data의 label을 생성해 줍니다.\n",
    "\n",
    "train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n",
    "                        np.ones(len(australia_train_data), dtype = np.int),\n",
    "                         np.ones(len(canada_train_data), dtype = np.int) * 2,\n",
    "                         np.ones(len(england_train_data), dtype = np.int) * 3,\n",
    "                         np.ones(len(hongkong_train_data), dtype = np.int) * 4,\n",
    "                         np.ones(len(us_train_data), dtype = np.int) * 5), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hUZcmisKYuWk",
    "outputId": "1071c2e2-097a-4de0-f5fd-0ad35cda8892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25520, 64, 501, 1), (25520,), (6100, 64, 501, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yww2MSkqYuWk"
   },
   "source": [
    "### 분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1kIaKSdYuWk"
   },
   "source": [
    " 분석 모델은 월간데이콘_6 음성 중첩 데이터 분류 AI 경진대회 3위를 달성하신 Jamm님의 코드를 바탕으로 만들어졌습니다.  \n",
    " https://www.dacon.io/competitions/official/235616/codeshare/1571?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iAR1wUvrYuWl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n",
    "                                     Dropout, Dense, AveragePooling2D, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xz7eUJOWYuWl"
   },
   "outputs": [],
   "source": [
    "def block(input_, units = 32, dropout_rate = 0.5):\n",
    "    \n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
    "    \n",
    "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZCK7r8rpYuWl"
   },
   "outputs": [],
   "source": [
    "def build_fn():\n",
    "    dropout_rate = 0.3\n",
    "    \n",
    "    in_ = Input(shape = (train_x.shape[1:]))\n",
    "    \n",
    "    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n",
    "    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n",
    "    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n",
    "\n",
    "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
    "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
    "\n",
    "    x = Flatten()(block_05)\n",
    "\n",
    "    x = Dense(units = 128, activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "\n",
    "    x = Dense(units = 128, activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x_res, x])\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "\n",
    "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
    "    model = Model(in_, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wePJIGYuWl"
   },
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aBT5ZtraYuWl",
    "outputId": "f3756ccd-16ed-4a9a-f439-bffc73d717d2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MSI\\anaconda3\\envs\\py36_tf114\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 20416 samples, validate on 5104 samples\n",
      "Epoch 1/8\n",
      " 5664/20416 [=======>......................] - ETA: 1:14:21 - loss: 1.9186 - acc: 0.3409"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1caa4a4e3849>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                  metrics = ['acc'])\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*******************************************************************\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36_tf114\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36_tf114\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36_tf114\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36_tf114\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "split = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)\n",
    "\n",
    "pred = []\n",
    "pred_ = []\n",
    "\n",
    "for train_idx, val_idx in split.split(train_x, train_y):\n",
    "    x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
    "\n",
    "    model = build_fn()\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.002),\n",
    "                 loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics = ['acc'])\n",
    "\n",
    "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = 8)\n",
    "    print(\"*******************************************************************\")\n",
    "    pred.append(model.predict(test_x))\n",
    "    pred_.append(np.argmax(model.predict(test_x), axis = 1))\n",
    "    print(\"*******************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPCTmJopYuWm"
   },
   "source": [
    "### 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "835VL5uBYuWm"
   },
   "outputs": [],
   "source": [
    "def cov_type(data):\n",
    "    return np.int(data)\n",
    "\n",
    "# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n",
    "# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n",
    "\n",
    "result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x : cov_type(x))\n",
    "\n",
    "result = pd.merge(sample_submission[\"id\"], result)\n",
    "result.columns = sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Mk1kNtkYuWm",
    "outputId": "3b2c6e88-65a8-43bd-ae54-12bbad8f393e"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UF3gBKmoYuWm"
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"DACON.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKeAjPSKYuWm"
   },
   "source": [
    "baseline은 참가자의 제출을 최우선 목표로 하고 있습니다.  \n",
    "창의적인 전처리 방법을 적용하고 훌륭한 분석 모델을 개발해 주세요  \n",
    "  \n",
    "감사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fopiwTmIYuWn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "English_voice_classification_Dacon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
